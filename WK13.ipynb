{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 13\n",
    "\n",
    "Classification and Training Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/image_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/nn_utils.py\n",
    "\n",
    "!wget -qO- https://github.com/PSAM-5020-2025S-A/5020-utils/releases/latest/download/lfw.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from torch import Tensor, nn\n",
    "\n",
    "from data_utils import LFWUtils, classification_error, display_confusion_matrix\n",
    "from image_utils import make_image\n",
    "from nn_utils import get_labels, get_num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Review\n",
    "\n",
    "Can we use NN models to do classification of images? Sure.\n",
    "\n",
    "The steps are the same, we just have to load image data and adapt the cost/loss function to calculate some kind of classification metric instead.\n",
    "\n",
    "We'll use the _Labeled Faces in the Wild_ dataset from last homework.\n",
    "\n",
    "The steps for setting up the classification model will be:\n",
    "\n",
    "- Load dataset and do any kind of pre-pre-processing\n",
    "- Split data into train/test datasets\n",
    "- Perform any kind of pre-processing\n",
    "- Split independent features and classification label and load them into `Tensors`\n",
    "- Build a NN model\n",
    "- Set up an optimizer\n",
    "- Pick a cost/loss function\n",
    "- Implement an evaluation function and any other kind of visualization that helps quantify/evaluate the model\n",
    "- Train model\n",
    "- Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and split Dataset\n",
    "\n",
    "The `LFWUtils.train_test_split(0.5)` function gives us some `Python` objects we can use to create our `Tensor`s.\n",
    "\n",
    "The `pixels` key gives us a list of the images' pixel data, and the `label` key gives us the images' label IDs.\n",
    "\n",
    "We don't have to do any normalization since the pixels will be in a known, well-defined, range of $[0 \\text{ - } 255]$.\n",
    "\n",
    "The only thing we have to do differently is cast the labels `Tensor` to `long`. This is to ensure the numbers in those `Tensor`s are whole numbers and don't have decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LFWUtils.train_test_split(0.5)\n",
    "\n",
    "x_train = Tensor(train[\"pixels\"])\n",
    "y_train = Tensor(train[\"labels\"]).long()\n",
    "\n",
    "x_test = Tensor(test[\"pixels\"])\n",
    "y_test = Tensor(test[\"labels\"]).long()\n",
    "\n",
    "print(\"Dataset Samples\")\n",
    "print(\"\\tTrain:\", len(x_train))\n",
    "print(\"\\tTest:\", len(x_test))\n",
    "\n",
    "print(\"\\nDataset Shape:\", list(x_train.shape))\n",
    "print(\"\\nSample Shape:\", list(x_train[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at data\n",
    "\n",
    "We can visualize some of the images, their text labels and label IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "id = train[\"labels\"][idx]\n",
    "print(id, LFWUtils.LABELS[id])\n",
    "display(make_image(train[\"pixels\"][idx], width=130))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Optimizer and Cost Function\n",
    "\n",
    "We'll start with the simplest kind of network again, with just an input and an output layer.\n",
    "\n",
    "The input layer has as many neurons as the number of pixels in each image, and the output layer has one neuron per possible class.\n",
    "\n",
    "It looks like this, and is juts like our regression network above, but has more output neurons:\n",
    "\n",
    "<img src=\"./imgs/linear_22100x26.jpg\" width=\"800px\"/>\n",
    "\n",
    "Our optimizer will be `SGD` again.\n",
    "\n",
    "Our cost function is a bit different. Previously, we used $L2$ distances to calculate the root mean square error of our regression predictions and used that value as the cost function for gradient descent.\n",
    "\n",
    "In order to use gradient descent for classification, we have to turn the discrete nature of our labels/classes and their errors into something that has smooth and integratable slopes.\n",
    "\n",
    "That's what the `CrossEntropyLoss()` function does for us. It looks at the outputs of our model and transforms the regression-type continuous values at our neuron outputs into class prediction probabilities in a way that gradient descent still works.\n",
    "\n",
    "The loss for a given prediction is calculated by the equation:\n",
    "$$-\\log\\left(\\frac{e^{y_c}}{\\sum_{i=0}^{C}e^{y_i}}\\right)$$\n",
    "\n",
    "Where, $y_c$ is the value of the output neuron that corresponds to the correct class of the prediction, and $y_i$ are all of the output neuron values. This equation, with its $e^y$ and $\\log()$, results in a very high value whenever the correct neuron is not activated, but gives us a $0$ (or a value close to $0$) when the correct output neuron is the most strongly activated neuron amongst a few activated neurons.\n",
    "\n",
    "More importantly, this equation is integratable and can be used in the calculation of gradients for our neuron parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(x_train.shape[1], len(y_train.unique()))\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Shapes\n",
    "\n",
    "We're just going to put the inputs through the model to check if the parameter shapes match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(x_train)\n",
    "\n",
    "print(\"Input shape:\", x_train.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "We can train the model now.\n",
    "\n",
    "The `x` and `y` variables below actually hold pixel and label information for $445$ images. We feed all of them into the network at once and predict all of the labels at the same time. We can then calculate loss, calculate the slope of the loss function, update model parameters, zero the gradients, and repeat.\n",
    "\n",
    "We'll add an evaluation function that we'll run every once in a while within the training loop. This evaluation function will run on all of our data, training and test, to see when/if the model starts to overfit. This evaluation function also gives us something a little more legible than the `CrossEntropyLoss()` value, which is the sum of the \"negative log likelihood\" of our predictions.\n",
    "\n",
    "We'll just use the `get_labels()` function from the `Homework12_utils` class. It will run our model on all samples of a dataset and extract the class value from the most-activated neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  # clear slopes in optimizer\n",
    "  optim.zero_grad()\n",
    "\n",
    "  # compute labels from model\n",
    "  labels_pred = model(x_train)\n",
    "\n",
    "  # calculate how wrong model is\n",
    "  loss = loss_fn(labels_pred, y_train)\n",
    "\n",
    "  # compute slopes for cost function\n",
    "  loss.backward()\n",
    "\n",
    "  # adjust model's parameters\n",
    "  optim.step()\n",
    "\n",
    "  # keep an eye on loss as we train\n",
    "  if e % 4 == 3:\n",
    "    train_predictions = get_labels(model, x_train)\n",
    "    test_predictions = get_labels(model, x_test)\n",
    "    train_error = classification_error(y_train, train_predictions)\n",
    "    test_error = classification_error(y_test, test_predictions)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train error: {train_error:.4f}, test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The loss/cost value can oscillate up and down a bit, but, overall should steadily decrease. This oscillation has to do with the `SGD` optimizer and how it makes some decisions that it sometimes has to undo, but overall, the model looks like it's learning.\n",
    "\n",
    "Do we want to keep running this cell until the loss gets really small?\n",
    "\n",
    "How low can we get our test error ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Layers\n",
    "\n",
    "Maybe we can improve our classification by adding some hidden layers.\n",
    "\n",
    "We might want to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  nn.Sequential(\n",
    "  nn.Linear(x_train.shape[1], x_train.shape[1] // 2),\n",
    "  nn.Sigmoid(),\n",
    "\n",
    "  nn.Linear(x_train.shape[1] // 2, x_train.shape[1] // 16),\n",
    "  nn.Sigmoid(),\n",
    "\n",
    "  nn.Linear(x_train.shape[1] // 16, x_train.shape[1] // 32),\n",
    "  nn.Sigmoid(),\n",
    "\n",
    "  nn.Linear(x_train.shape[1] // 32, len(y_train.unique())),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, this neural network has these many parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot.\n",
    "\n",
    "Our regular codespaces computer won't be able to optimize a network of this size in a reasonable amount of time.\n",
    "\n",
    "We saw in the homework that if we want to add layers we have to reduce the number of our features. So .... PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "pca = PCA(n_components=0.9999)\n",
    "\n",
    "x_pca_train = pca.fit_transform(std.fit_transform(train[\"pixels\"]))\n",
    "x_pca_test = pca.transform(std.transform(test[\"pixels\"]))\n",
    "\n",
    "print(pca.n_components_, pca.explained_variance_ratio_.sum())\n",
    "\n",
    "x_train = Tensor(x_pca_train)\n",
    "y_train = Tensor(train[\"labels\"]).long()\n",
    "\n",
    "x_test = Tensor(x_pca_test)\n",
    "y_test = Tensor(test[\"labels\"]).long()\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  nn.Sequential(\n",
    "  nn.Linear(x_train.shape[1], x_train.shape[1] // 2),\n",
    "  nn.Sigmoid(),\n",
    "\n",
    "  nn.Linear(x_train.shape[1] // 2, x_train.shape[1] // 8),\n",
    "  nn.Sigmoid(),\n",
    "\n",
    "  # nn.Linear(x_train.shape[1] // 16, x_train.shape[1] // 32),\n",
    "  # nn.Sigmoid(),\n",
    "\n",
    "  # nn.Linear(x_train.shape[1] // 32, len(y_train.unique())),\n",
    "  nn.Linear(x_train.shape[1] // 8, len(y_train.unique())),\n",
    ")\n",
    "\n",
    "learning_rate = 1e-1\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "out = model(x_train)\n",
    "\n",
    "print(\"Input shape:\", x_train.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Num Params:\", get_num_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(256):\n",
    "  optim.zero_grad()\n",
    "  labels_pred = model(x_train)\n",
    "  loss = loss_fn(labels_pred, y_train)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "\n",
    "  if e % 32 == 31:\n",
    "    train_predictions = get_labels(model, x_train)\n",
    "    test_predictions = get_labels(model, x_test)\n",
    "    train_error = classification_error(y_train, train_predictions)\n",
    "    test_error = classification_error(y_test, test_predictions)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train error: {train_error:.4f}, test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = get_labels(model, x_train)\n",
    "test_predictions = get_labels(model, x_test)\n",
    "\n",
    "print(\"train error:\", f\"{classification_error(y_train, train_predictions):.4f}\")\n",
    "print(\"test error\", f\"{classification_error(y_test, test_predictions):.4f}\")\n",
    "\n",
    "display_confusion_matrix(y_train, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(y_test, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The result is mostly the same, which is not surprising.\n",
    "\n",
    "We did add layers, but the network didn't need any extra neurons to do well on the training data.\n",
    "\n",
    "It needs help with the testing data, or, another way to say this is: it needs help generalizing without memorizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make It Harder\n",
    "\n",
    "Neural network models can seem simple to explain in a general sense: they're long and wide computation graphs made up of simple operations that have been tuned to achieve a specific task. Once they're training, or trained, their details and specificities are a little less easy to describe. It's hard to know exactly what each neuron is doing, and what part of the computation they are responsible for. We can train the same network, with the same parameters, using the same input data, and end up with wildly different results.\n",
    "\n",
    "This is one reason why it's hard to debug a network when it doesn't seem to be learning properly, or when it starts to overfit and memorize the training data. Which neurons do we tune ?\n",
    "\n",
    "One common situation that can lead to overfitting is when a network ends up with parameters that make it perform well on the training data without really activating all of its neurons. This is usually what is happening if adding layers to a network doesn't improve its performance.\n",
    "\n",
    "One set of strategies for improving neural network training in these cases involves making the training process harder than it has to be. It's like we're challenging the neural network to learn more than it has, so that later it has an easier time with the regular data.\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "One simple technique to achieve this is to add `Dropout` layers to our network. A `Dropout` layer is a layer of neurons that don't perform any mathematical operation, but are selectively dropped out of the network randomly during training. This has the effect of randomly changing the network's architecture during training and preventing the network from becoming too reliant on specific neurons. Instead, it encourages the network to learn more robust features by activating more diverse sets of neurons.\n",
    "\n",
    "<img src=\"./imgs/dropout.jpg\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Normalization\n",
    "\n",
    "Another technique that is used to keep our neural networks from memorizing data has to do with the range of the values that get passed between its inner layers.\n",
    "\n",
    "Input data coming into the network is most likely normalized, but after the first layer, the network weights might really change the distribution of the data as it flows through the network. Moreover, individual batches with different input value distributions can bias the network towards certain goals.\n",
    "\n",
    "<img src=\"./imgs/norm_activation.jpg\" width=\"720px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization\n",
    "\n",
    "One way to handle these situations is to normalize the data as it passes through the network. Batch Normalization is the process of normalizing the activations of our network by using the mean and standard deviation of an activation neuron across a batch. The result is that the activations between batches become more similar. Batch normalization is dependent on batch size, so it's not effective for small batches.\n",
    "\n",
    "<img src=\"./imgs/norm_batch.jpg\" width=\"720px\"/>\n",
    "\n",
    "#### Layer Normalization\n",
    "\n",
    "Another form of inner-network normalization can be added to make sure no individual layer overpowers the network with activation values that are too large or too small.\n",
    "\n",
    "Layer Normalization scales activations using the mean and standard deviation of all activations across a layer. It's effective for sequence models like RNNs and Transformers, and for scenarios with small batch sizes, and doesn't require a large batch to get a good estimate for mean and standard deviation. \n",
    "\n",
    "<img src=\"./imgs/norm_layer.jpg\" width=\"720px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  nn.Sequential(\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(x_train.shape[1], x_train.shape[1] // 2),\n",
    "  nn.BatchNorm1d(x_train.shape[1] // 2),\n",
    "  nn.LayerNorm(x_train.shape[1] // 2),\n",
    "  nn.Sigmoid(),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(x_train.shape[1] // 2, x_train.shape[1] // 16),\n",
    "  nn.BatchNorm1d(x_train.shape[1] // 16),\n",
    "  nn.LayerNorm(x_train.shape[1] // 16),\n",
    "  nn.Sigmoid(),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(x_train.shape[1] // 16, x_train.shape[1] // 32),\n",
    "  nn.BatchNorm1d(x_train.shape[1] // 32),\n",
    "  nn.LayerNorm(x_train.shape[1] // 32),\n",
    "  nn.Sigmoid(),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(x_train.shape[1] // 32, len(y_train.unique())),\n",
    ")\n",
    "\n",
    "learning_rate = 1e-1\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "out = model(x_train)\n",
    "\n",
    "print(\"Input shape:\", x_train.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Num Params:\", get_num_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(256):\n",
    "  optim.zero_grad()\n",
    "  labels_pred = model(x_train)\n",
    "  loss = loss_fn(labels_pred, y_train)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "\n",
    "  if e % 32 == 31:\n",
    "    train_predictions = get_labels(model, x_train)\n",
    "    test_predictions = get_labels(model, x_test)\n",
    "    train_error = classification_error(y_train, train_predictions)\n",
    "    test_error = classification_error(y_test, test_predictions)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train error: {train_error:.4f}, test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The train and test eval function diverged, but both keep decreasing, so this might be ok.\n",
    "\n",
    "In the end, the network seems capable of learning for longer and while the classification error for the test dataset doesn't keep up with the error in the train dataset, it does perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = get_labels(model, x_train)\n",
    "test_predictions = get_labels(model, x_test)\n",
    "\n",
    "print(\"train error\", f\"{classification_error(y_train, train_predictions):.4f}\")\n",
    "print(\"test error\", f\"{classification_error(y_test, test_predictions):.4f}\")\n",
    "\n",
    "display_confusion_matrix(y_train, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(y_test, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "9103",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
